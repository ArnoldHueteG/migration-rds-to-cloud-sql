import json
import logging
import multiprocessing
import time
from datetime import datetime
import pickle
from pathlib import Path

from googleapiclient import discovery, errors
from oauth2client.client import GoogleCredentials

import psycopg2
from local.secrets.environment_vars import BD_CONNECTION_1


debug_mode = False


class DataMigrationService:
    """Client to bundle configuration needed for API requests.

    https://cloud.google.com/database-migration/docs/apis

    Args:
            prefix_dict (dict of str: str): dict with prefixes to use naming the connection profiles and migration jobs.
                prefix_cp_source str : prefix use by the name of connection profile for the source.
                prefix_cp_cloudsql str : prefix use by the name of connection profile for the cloud sql instance.
                prefix_mj str : prefix use by the name of migration job.
                id int : id use as a differentiator for the migration job name.
                TODO: Add autogenerated id in case the migration job name already exist.
                For example:
                prefix_dict = {
                                "prefix_cp_source": "auto-cp-pg-",
                                "prefix_cp_cloudsql": "auto-cs-pg-",
                                "prefix_mj": "auto-mj-",
                                "id": 0,
                            }
            location_dict (dict of str: str): dict with location of cliente (project_id, region_id)
                project_id str: the project which the client acts on behalf of. Will be passed in requests.
                region_id str: The location of the bucket. See
                https://cloud.google.com/storage/docs/bucket-locations
                For example:
                location_dict = {
                                    "project_id": "aws-rds-gcp-cloudsql",
                                    "region_id": "us-east4"
                                }
            source_connection (dict of str: dict): dict with connection parameters.
                postgresql (dict of str: str): dict with connection parameters for postgresql
                    host str: Required. The IP or hostname of the source PostgreSQL database.
                    port str: Required. The network port of the source PostgreSQL database.
                    username str: Required. The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
                    password str: Required. Input only. The password for the user that Database Migration Service will be using to connect to the database. This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
                For example:
                    source_connection = {
                        "postgresql": {
                            "host": "HOST",
                            "port": "PORT",
                            "username": "USER",
                            "password": "PASSWORD",
                        }
                    }
            target_base_settings_cloud_sql (dict of str: str): Settings for creating a Cloud SQL database instance.
                ipConfig (dict of str: str): The settings for IP Management. This allows to enable or disable the instance IP and manage which external networks can connect to the instance. The IPv4 address cannot be disabled.
                autoStorageIncrease boolean: [default: ON] If you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity. If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB.
                dataDiskType str: The type of storage: PD_SSD (default) or PD_HDD.
                rootPassword boolean: Output only. Indicates If this connection profile root password is stored.
                For example:
                target_base_settings_cloud_sql = {
                    "ipConfig": {"enableIpv4": True},
                    "autoStorageIncrease": True,
                    "dataDiskType": "PD_SSD",
                    "rootPassword": "postgres",
                }
                TODO: change this to class variable.
            target_server_settings_cloud_sql (dict of str: str): Settings for creating a Cloud SQL database instance.
                databaseVersion str: The database engine type and version [POSTGRES_9_6,POSTGRES_10,POSTGRES_11,POSTGRES_12,POSTGRES_13]
                tier str: The tier (or machine type) for this instance, for example: db-n1-standard-1 (MySQL instances) or db-custom-1-3840 (PostgreSQL instances). For more information, see https://cloud.google.com/sql/docs/mysql/instance-settings.
                dataDiskSizeGb int: The storage capacity available to the database, in GB. The minimum (and default) size is 10GB.
                For example:
                target_server_settings_cloud_sql = {
                    "databaseVersion": "POSTGRES_12",
                    "tier": "db-custom-1-3840",
                    "dataDiskSizeGb": 15,
                }
    """

    def __init__(
        self,
        prefix_dict,
        location_dict,
        source_connection,
        target_base_settings_cloud_sql,
        target_server_settings_cloud_sql,
    ):
        """[summary]"""
        self.prefix_dict = prefix_dict
        self.location_dict = location_dict
        self.source_connection = source_connection
        self.target_base_settings_cloud_sql = target_base_settings_cloud_sql
        self.target_server_settings_cloud_sql = target_server_settings_cloud_sql
        self.prefix_cp_source = prefix_dict["prefix_cp_source"]
        self.prefix_cp_cloudsql = prefix_dict["prefix_cp_cloudsql"]
        self.prefix_mj = prefix_dict["prefix_mj"]
        # self.now_str = prefix_dict["now_str"]
        self.now_str = datetime.now().strftime("%Y%m%dt%H%M%S")
        self.id = prefix_dict["id"]
        self.project_id = location_dict["project_id"]
        self.region_id = location_dict["region_id"]
        self.rds_name = source_connection["postgresql"]["host"].split(".")[0]
        self.connection_profile_id_source = "{1}{0}-{2}-{3}".format(
            self.id, self.prefix_cp_source, self.rds_name, self.now_str
        )
        self.connection_profile_id_cloudsql = "{1}{0}-{2}-{3}".format(
            self.id, self.prefix_cp_cloudsql, self.rds_name, self.now_str
        )
        self.migration_job_id = "{1}{0}-{2}-{3}".format(
            self.id, self.prefix_mj, self.rds_name, self.now_str
        )
        self.logfile_name = "data/output/logs/{3}-{1}{0}-{2}.log".format(
            self.id, self.prefix_mj, self.rds_name, self.now_str
        )
        # self.logfile_name = "logs/{1}{0}-{2}.log".format(self.id, self.prefix_mj, self.rds_name)
        self.picklefile_name = f"data/output/pickles/{self.id}-{self.rds_name}.pkl"
        self.logger = self.setup_logger(
            f"{self.id} - {self.rds_name}", self.logfile_name
        )
        self.logger.setLevel(logging.INFO)
        Path("/".join(self.picklefile_name.split("/")[:-1])).mkdir(parents=True, exist_ok=True)
        with open(self.picklefile_name, "wb") as output:
            pickle.dump(self, output)

    def setup_logger(self, logger_name=None, logfile_name=None):
        Path("/".join(logfile_name.split("/")[:-1])).mkdir(parents=True, exist_ok=True)
        logger = logging.getLogger(logger_name)

        formatter = logging.Formatter(
            "%(asctime)s [ %(name)s ] %(levelname)s : %(message)s",
            datefmt="%Y/%m/%d %H:%M:%S",
        )

        if logfile_name is not None:
            fileHandler = logging.FileHandler(logfile_name, mode="w")
            fileHandler.setFormatter(formatter)
            logger.addHandler(fileHandler)

        streamHandler = logging.StreamHandler()
        streamHandler.setFormatter(formatter)
        logger.addHandler(streamHandler)

        logger.setLevel(logging.DEBUG)
        return logger

    def generate_migration_job(self):
        """[summary]"""
        self.logger.info("Starting migration job")
        if not self.test_connection():
            self.logger.info(
                "migration job won't continue because connection test was not succesful"
            )
            return
        request_dict = {
            "displayName": self.connection_profile_id_source,
            **self.source_connection,
        }
        self.logger.info("Creating profile in dms service")
        self.create_connection_profile(
            request_dict=request_dict, wait_to_be_ready=False
        )
        request_dict = {
            "displayName": self.connection_profile_id_cloudsql,
            "cloudsql": {
                "settings": {
                    **self.target_base_settings_cloud_sql,
                    **self.target_server_settings_cloud_sql,
                    "sourceId": "projects/{}/locations/{}/connectionProfiles/{}".format(
                        self.project_id,
                        self.region_id,
                        self.connection_profile_id_source,
                    ),
                }
            },
        }
        self.logger.info("Creating instance for replication")
        self.create_connection_profile(
            request_dict=request_dict, wait_to_be_ready=True
        )
        request_dict = {
            "type": "CONTINUOUS",
            "source": "projects/{}/locations/{}/connectionProfiles/{}".format(
                self.project_id,
                self.region_id,
                self.connection_profile_id_source,
            ),
            "destination": "projects/{}/locations/{}/connectionProfiles/{}".format(
                self.project_id,
                self.region_id,
                self.connection_profile_id_cloudsql,
            ),
            "destinationDatabase": {"provider": "RDS", "engine": "POSTGRESQL"},
        }
        self.logger.info("Creating migration job for replication")
        self.create_migration_job(request_dict=request_dict)

    def test_connection(self):
        source_connection = dict(self.source_connection["postgresql"])
        source_connection["database"] = "postgres"
        try:
            self.logger.info("Testing connection")
            conn = psycopg2.connect(
                dbname=source_connection["database"],
                host=source_connection["host"],
                user=source_connection["username"],
                password=source_connection["password"],
                port=source_connection["port"],
            )
            conn.close()
            self.logger.info(
                "Your connection to {} was successful.".format(
                    source_connection["host"]
                )
            )
            return True
        except Exception as e:
            self.logger.info(
                "Your connection to {} was not successful.".format(
                    source_connection["host"]
                )
            )
            self.logger.debug("{}".format(e).strip())
            return False

    def create_connection_profile(self, request_dict, wait_to_be_ready=False):
        connection_profile_id = request_dict["displayName"]
        dms = discovery.build("datamigration", "v1")
        response = request_dict
        if not debug_mode:
            response = (
                dms.projects()
                .locations()
                .connectionProfiles()
                .create(
                    parent="projects/{}/locations/{}".format(
                        self.project_id, self.region_id
                    ),
                    connectionProfileId=connection_profile_id,
                    body=request_dict,
                )
                .execute()
            )
        self.logger.debug(json.dumps(response, indent=2))
        if wait_to_be_ready and not debug_mode:
            response = {}
            while True:
                response = (
                    dms.projects()
                    .locations()
                    .connectionProfiles()
                    .get(
                        name="projects/{}/locations/{}/connectionProfiles/{}".format(
                            self.project_id,
                            self.region_id,
                            connection_profile_id,
                        )
                    )
                    .execute()
                )
                self.logger.debug(response.get("state"))
                if response.get("state") != "READY":
                    time.sleep(20)
                else:
                    break

    def create_migration_job(self, request_dict):
        dms = discovery.build("datamigration", "v1")
        dms.migration_jobs = dms.projects().locations().migrationJobs()
        response = request_dict
        if not debug_mode:
            response = dms.migration_jobs.create(
                parent="projects/{}/locations/{}".format(
                    self.project_id, self.region_id
                ),
                migrationJobId=self.migration_job_id,
                body=request_dict,
            ).execute()
        self.logger.debug(json.dumps(response, indent=2))
        response = {}
        if not debug_mode:
            while True:
                response = dms.migration_jobs.get(
                    name="projects/{}/locations/{}/migrationJobs/{}".format(
                        self.project_id, self.region_id, self.migration_job_id
                    )
                ).execute()
                self.logger.debug(response.get("state"))
                if response.get("state") != "NOT_STARTED":
                    time.sleep(5)
                else:
                    break
            response = dms.migration_jobs.start(
                name="projects/{}/locations/{}/migrationJobs/{}".format(
                    self.project_id, self.region_id, self.migration_job_id
                )
            ).execute()
            self.logger.debug(json.dumps(response, indent=2))
        response = {}
        if not debug_mode:
            while True:
                response = dms.migration_jobs.get(
                    name="projects/{}/locations/{}/migrationJobs/{}".format(
                        self.project_id, self.region_id, self.migration_job_id
                    )
                ).execute()
                self.logger.debug(response.get("state"))
                if response.get("state") != "RUNNING":
                    time.sleep(10)
                else:
                    break

    def get_state_from_migration_job(self):
        dms = discovery.build("datamigration", "v1")
        dms.migration_jobs = dms.projects().locations().migrationJobs()
        try:
            response = dms.migration_jobs.get(
                name="projects/{}/locations/{}/migrationJobs/{}".format(
                    self.project_id, self.region_id, self.migration_job_id
                )
            ).execute()
            state = response.get("state")
        except Exception as e:
            state = e.resp.reason.upper()
        response = {
            "host": self.rds_name,
            "migration_job_id": self.migration_job_id,
            "state": state,
        }
        self.logger.debug(json.dumps(response, indent=2))
        return response


if __name__ == "__main__":
    prefix_dict = {
        "prefix_cp_source": "auto-cp-pg-",
        "prefix_cp_cloudsql": "auto-cs-pg-",
        "prefix_mj": "auto-mj-",
        "id": 0,
    }
    location_dict = {
        "project_id": "dceng-testing-au-data",
        "region_id": "us-east4",
    }
    source_connection = {
        "postgresql": {
            "host": BD_CONNECTION_1.split(":")[0],
            "port": BD_CONNECTION_1.split(":")[1],
            "username": BD_CONNECTION_1.split(":")[2],
            "password": BD_CONNECTION_1.split(":")[3],
        }
    }
    target_base_settings_cloud_sql = {
        "ipConfig": {"enableIpv4": True},
        "autoStorageIncrease": True,
        "dataDiskType": "PD_SSD",
        "rootPassword": "postgres",
    }
    target_server_settings_cloud_sql = {
        "databaseVersion": "POSTGRES_12",
        "tier": "db-custom-1-3840",
        "dataDiskSizeGb": 15,
    }
    dms = DataMigrationService(
        prefix_dict,
        location_dict,
        source_connection,
        target_base_settings_cloud_sql,
        target_server_settings_cloud_sql,
    )
    dms.logger.setLevel(logging.DEBUG)
    dms.generate_migration_job()
